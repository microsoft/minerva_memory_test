import json
import logging
import re
from typing import Dict, List, Set, Union, Any

from rouge_score import rouge_scorer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
COUNT_WORDS = {
    "once": 1, "twice": 2, "three": 3, "thrice": 3, "four": 4,
    "five": 5, "six": 6, "seven": 7, "eight": 8, "nine": 9, "ten": 10,
}


def evaluate_generation(generation: str, reference: Any, metrics: List[str]) -> Dict[str, float]:
    """Evaluate a model's generation against a reference using specified metrics.

    Args:
        generation: The text generated by the model
        reference: The expected answer (string, list, dict, etc.)
        metrics: List of metrics to compute

    Returns:
        Dictionary containing scores for each metric
    """
    scores = {}
    if generation is None:
        generation = ""
        
    for metric in metrics:
        if metric == "exact_match":
            score = compute_exact_match(reference, generation)
        elif metric == "rouge":
            score = compute_rouge(reference, generation)
        elif metric == "count_accuracy":
            score = compute_count_accuracy(reference, generation)
        elif metric == "set_overlap":
            score = compute_set_overlap_accuracy(reference, generation)
        elif metric == "theory_of_mind":
            score = compute_theory_of_mind_accuracy(reference, generation)
        elif metric == "final_answer_exact_match":
            score = compute_final_answer_exact_match(reference, generation)
        else:
            logger.warning(f"Unknown metric: {metric}")
            continue
        scores.update(score)
    return scores


def compute_average_score(evaluation_filepath: str) -> Dict[str, float]:
    """Compute average scores across all examples in an evaluation file.
    
    Args:
        evaluation_filepath: Path to JSONL file with evaluation results
        
    Returns:
        Dictionary of average scores for each metric
    """
    with open(evaluation_filepath, "r") as f:
        scores = [json.loads(line)["scores"] for line in f]

    if not scores:
        return {}
        
    average_scores = {}
    for metric in scores[0]:
        average_scores[metric] = sum(score[metric] for score in scores) / len(scores)
    return average_scores


def format_reference(reference: Any) -> str:
    """Format reference answer to string for comparison.
    
    Args:
        reference: Reference answer in various formats
        
    Returns:
        Formatted string representation
    """
    if isinstance(reference, list):
        return ", ".join(str(r) for r in reference)
    elif isinstance(reference, dict):
        output = ""
        for key, value in reference.items():
            output += f"{key}: {', '.join(value)}\n"
        return output
    elif isinstance(reference, str):
        return reference.lower()
    elif isinstance(reference, int):
        return str(reference)
    return str(reference)


def compute_exact_match(reference: Any, generation: str) -> Dict[str, float]:
    """Compute exact match score.
    
    Args:
        reference: Expected answer
        generation: Model's generated answer
        
    Returns:
        Dictionary with exact_match score (1.0 or 0.0)
    """
    reference = format_reference(reference)
    generation = generation.strip().lower().rstrip(".")
    return {"exact_match": 1.0 if reference in generation else 0.0}


def compute_rouge(reference: Any, generation: str) -> Dict[str, float]:
    """Compute ROUGE scores between reference and generation.
    
    Args:
        reference: Expected answer
        generation: Model's generated answer
        
    Returns:
        Dictionary with various ROUGE metrics
    """
    reference = format_reference(reference)
    scorer = rouge_scorer.RougeScorer(["rouge1", "rougeL"], use_stemmer=False)
    scores = scorer.score(reference, generation)
    return {
        "rouge-1": scores["rouge1"].fmeasure,
        "rouge-L": scores["rougeL"].fmeasure,
        "rouge-1_precision": scores["rouge1"].precision,
        "rouge-1_recall": scores["rouge1"].recall,
        "rouge-L_precision": scores["rougeL"].precision,
        "rouge-L_recall": scores["rougeL"].recall,
    }


def compute_count_accuracy(reference: int, generation: str) -> Dict[str, float]:
    """Compute accuracy for count tasks.
    
    Args:
        reference: Expected count
        generation: Model's generated answer
        
    Returns:
        Dictionary with count_accuracy score (1.0 or 0.0)
    """
    generation_count = None
    
    # Try to find count words (once, twice, etc.)
    for word, count in COUNT_WORDS.items():
        if word in generation:
            generation_count = count
            break
    
    # If no count word found, try to find numeric pattern
    if generation_count is None:
        match = re.search(r"(\d+) times", generation)
        if match:
            generation_count = int(match.group(1))
    
    return {"count_accuracy": 1.0 if int(reference) == generation_count else 0.0}


def parse_final_answer(generation: str) -> str:
    """Extract final answer from generation.
    
    Args:
        generation: Model's generated text
        
    Returns:
        Extracted final answer
    """
    match = re.search(r"FINAL ANSWER:(.+)", generation, re.DOTALL)
    if match:
        final_answer = match.group(1).strip().rstrip(".")
    else:
        logger.debug(f"Could not find FINAL ANSWER in generation: {generation}")
        final_answer = generation.strip().rstrip(".")
    
    return final_answer


def calculate_set_overlap(reference_set: Set, generation_set: Set) -> Dict[str, float]:
    """Calculate overlap metrics between two sets.
    
    Args:
        reference_set: Set of reference items
        generation_set: Set of generated items
        
    Returns:
        Dictionary with precision, recall, F1, and Jaccard similarity
    """
    intersection = reference_set & generation_set
    union = reference_set | generation_set

    precision = len(intersection) / len(generation_set) if generation_set else 0.0
    recall = len(intersection) / len(reference_set) if reference_set else 0.0
    
    if precision + recall > 0:
        f1 = 2 * (precision * recall) / (precision + recall)
    else:
        f1 = 0.0

    jaccard = len(intersection) / len(union) if union else 0.0

    return {
        "set_overlap_precision": precision,
        "set_overlap_recall": recall,
        "set_overlap_f1": f1,
        "set_overlap_jaccard_similarity": jaccard,
    }


def compute_set_overlap_accuracy(reference: List, generation: str) -> Dict[str, float]:
    """Compute set overlap metrics between reference and generation.
    
    Args:
        reference: List of expected items
        generation: Model's generated text
        
    Returns:
        Dictionary with set overlap metrics
    """
    generation = parse_final_answer(generation)
    reference_set = set(reference)
    generation_set = set(generation.split(", "))

    return calculate_set_overlap(reference_set, generation_set)


def parse_final_answer_tom(generation: str) -> Dict[str, Set[str]]:
    """Parse theory of mind final answer format.
    
    Args:
        generation: Model's generated text
        
    Returns:
        Dictionary mapping agent names to their knowledge sets
    """
    match = re.search(r"FINAL ANSWER:(.+)", generation, re.DOTALL)
    if match:
        final_answer = match.group(1).strip().rstrip(".")
    else:
        logger.debug(f"Could not find FINAL ANSWER in generation: {generation}")
        final_answer = generation.strip()
    
    agents = final_answer.split("\n")
    result = {}
    
    for i, agent_line in enumerate(agents):
        parts = agent_line.split(":", 1)
        if len(parts) != 2:
            logger.debug(f"Could not parse agent output at line {i}: {agent_line}")
            continue
            
        agent_name = parts[0].strip()
        agent_list = [x.strip() for x in parts[1].split(",")]
        result[agent_name] = set(agent_list)

    return result


def compute_theory_of_mind_accuracy(reference: Dict[str, List[str]], 
                                   generation: str) -> Dict[str, float]:
    """Compute theory of mind accuracy metrics.
    
    Args:
        reference: Dictionary mapping agent names to expected knowledge
        generation: Model's generated text
        
    Returns:
        Dictionary with average set overlap metrics across agents
    """
    generation = parse_final_answer_tom(generation)
    agent_scores = []
    
    for agent, reference_list in reference.items():
        reference_set = set(reference_list)
        if agent in generation:
            generation_set = generation[agent]
            agent_accuracy = calculate_set_overlap(reference_set, generation_set)
        else:
            agent_accuracy = {
                "set_overlap_precision": 0.0,
                "set_overlap_recall": 0.0,
                "set_overlap_f1": 0.0,
                "set_overlap_jaccard_similarity": 0.0,
            }
        agent_scores.append(agent_accuracy)

    # Average scores across agents
    avg_scores = {
        k: sum(score[k] for score in agent_scores) / len(agent_scores) 
        for k in agent_scores[0]
    }
    
    return avg_scores


def compute_final_answer_exact_match(reference: Any, generation: str) -> Dict[str, float]:
    """Compute exact match on final answer.
    
    Args:
        reference: Expected answer
        generation: Model's generated text
        
    Returns:
        Dictionary with exact_match score (1.0 or 0.0)
    """
    generation = parse_final_answer(generation)
    reference = format_reference(reference)
    return {"exact_match": 1.0 if reference == generation else 0.0}


if __name__ == "__main__":
    # Example usage
    reference = 5
    generation = "5 \nFINAL ANSWER: 5."

    metrics = ["final_answer_exact_match"]
    scores = evaluate_generation(generation, reference, metrics)
    print(scores)